## Gemma2-2b-it JOBS: Fine-tuning and Inference

[ this script contains all jobs that were run for fine-tuning and evaluation for our ICLR'2025 paper https://arxiv.org/abs/2410.01639]
[ comments are written in square brackets '[]' ]
[ each fine-tuning job was run on a single A100 or V100 GPU node, using up to 32GB RAM, and taking up to 5 hrs (vs static opponent) or 6.5 hrs (vs another learning LLM opponent) ] 
[ recommendation: run each fine-tuning job in a separate .sh script ] 
[ note: for fine-tuning you will need to specify your WANDB_API token; for fine-tuning and inference you will need to specify an HF_token] 

##################################################################
#### FINE-TUNING JOBS ####
##################################################################
[PT2 = fine-tuning with Game Reward
PT3 = fine-tuning with De reward 9default) or Ut reward (if specified in script) 
PT4 = fine-tuning with Game + De reward 
PT2then3 = fine-tuning with Game reward for 500 episodes, then with Ut/De reward (as specified) for 500 episodes] 

[parameters: 
--run defines the seed for the run 
--do_PART2 (or _PART3/4) defines which reward definition to use; if both --do_PART2 and --do_PART3 are True, we need to specify --num_episodes to 500 instead of 1000 because it will run each part for the specified duration
--moral_type is an optional parameter used to define if we want to run training with the Ut reward (Rather than the De default). These only come into play in PART3 or PART2then3.
--option is a parameter set for storing trained models 
--CD_tokens is the parameter used to define which symbols to use to represent the action tokens. 
]

[run fine-tuning - all parts vs TFT agent]
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 2 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 3 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 4 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 5 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 6 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 2 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 3 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 5 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 6 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 2 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 3 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 5 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 6 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 2 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 3 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 5 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 6 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 1 --batch_size 5 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 2 --batch_size 5 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 3 --batch_size 5 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 4 --batch_size 5 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 5 --batch_size 5 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 6 --batch_size 5 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 1 --batch_size 5 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 2 --batch_size 5 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 3 --batch_size 5 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 5 --batch_size 5 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 6 --batch_size 5 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

[run fine-tuning - key parts (PART2, 3-De, 3-Ut, 4) vs Random agent]
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 2 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 3 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 4 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 5 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 6 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 2 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 3 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 5 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 6 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 2 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 3 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 5 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 6 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 2 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 3 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 5 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 6 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

[run fine-tuning - key parts (PART2, 3-De, 3-Ut, 4) vs AD agent]
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 2 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 3 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 4 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 5 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 6 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 2 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 3 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 5 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 6 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 2 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 3 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 5 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 6 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 2 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 3 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 5 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AD --run 6 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

[run fine-tuning - key parts (PART2, 3-De, 3-Ut, 4) vs AC agent]
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 2 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 3 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 4 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 5 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 6 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 2 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 3 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 5 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 6 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 2 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 3 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 5 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 6 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 2 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 3 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 5 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune.py --base_model_id "google/gemma-2-2b-it" --opp_strat AC --run 6 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4



[run CORE - all parts vs LLM agent, NB batch size of 3 instead of 5 (to save GPU RAM)]
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 1 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 2 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 3 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 4 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 5 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 6 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 1 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "De" --LoRA True --do_PART3 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 2 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "De" --LoRA True --do_PART3 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 3 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "De" --LoRA True --do_PART3 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 5 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "De" --LoRA True --do_PART3 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 6 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "De" --LoRA True --do_PART3 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 1 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "Ut" --LoRA True --do_PART3 True --option "CORE-2learnersUt" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 2 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "Ut" --LoRA True --do_PART3 True --option "CORE-2learnersUt" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 3 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "Ut" --LoRA True --do_PART3 True --option "CORE-2learnersUt" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 5 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "Ut" --LoRA True --do_PART3 True --option "CORE-2learnersUt" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 6 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "Ut" --LoRA True --do_PART3 True --option "CORE-2learnersUt" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 1 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 2 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 3 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 5 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 6 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 1 --batch_size 3 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "De" --LoRA True --do_PART2 True --do_PART3 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 2 --batch_size 3 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "De" --LoRA True --do_PART2 True --do_PART3 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 3 --batch_size 3 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "De" --LoRA True --do_PART2 True --do_PART3 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 5 --batch_size 3 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "De" --LoRA True --do_PART2 True --do_PART3 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 6 --batch_size 3 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "De" --LoRA True --do_PART2 True --do_PART3 True --option "CORE-2learners" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 1 --batch_size 3 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "Ut" --LoRA True --do_PART2 True --do_PART3 True --option "CORE-2learnersUt" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 2 --batch_size 3 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "Ut" --LoRA True --do_PART2 True --do_PART3 True --option "CORE-2learnersUt" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 3 --batch_size 3 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "Ut" --LoRA True --do_PART2 True --do_PART3 True --option "CORE-2learnersUt" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 5 --batch_size 3 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "Ut" --LoRA True --do_PART2 True --do_PART3 True --option "CORE-2learnersUt" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 6 --batch_size 3 --num_episodes 500 --payoff_version "smallerR" --CD_tokens "action12" --moral_type "Ut" --LoRA True --do_PART2 True --do_PART3 True --option "CORE-2learnersUt" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4


[extra fine-tuning experiments - try different parameters in PART2]
python /src/fine_tune_TRL_core.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --r_illegal 100 --r_punishment 83 --LoRA_rank 4 
python /src/fine_tune_TRL_core.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --r_illegal 6 --r_punishment 3 --LoRA_rank 4 --QLoRA True
python /src/fine_tune_TRL_core.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --r_illegal 100 --r_punishment 83 --LoRA_rank 64 --LoRA_alpha 128
python /src/fine_tune_TRL_core.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --init_kl_coef 0.4 --adap_kl_ctrl True
python /src/fine_tune_TRL_core.py --base_model_id "google/gemma-2-2b-it" --opp_strat TFT --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --option "CORE" --gradient_accumulation_steps 4


[extra fine-tuning experiments - use a much lower R_illegal - try on PART4] 
python /src/fine_tune_TRL_core_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 1 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE-2learners" --Rscaling True --r_illegal 15 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_TRL_core_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 2 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE-2learners" --Rscaling True --r_illegal 15 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_TRL_core_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 3 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE-2learners" --Rscaling True --r_illegal 15 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_TRL_core_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 5 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE-2learners" --Rscaling True --r_illegal 15 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_TRL_core_2learners.py --base_model_id "google/gemma-2-2b-it" --opp_strat LLM --run 6 --batch_size 3 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART4 True --option "CORE-2learners" --Rscaling True --r_illegal 15 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4


[extra long fine-tuning experiments for unlearning - not reported in the paper] 
python /src/fine_tune_TRL_core.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_TRL_core.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 2 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_TRL_core.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 3 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_TRL_core.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 4 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_TRL_core.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 5 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_TRL_core.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 6 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type "De" --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4

python /src/fine_tune_TRL_core.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 1 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_TRL_core.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 2 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_TRL_core.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 3 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_TRL_core.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 5 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4
python /src/fine_tune_TRL_core.py --base_model_id "google/gemma-2-2b-it" --opp_strat Random --run 6 --batch_size 5 --num_episodes 1000 --payoff_version "smallerR" --CD_tokens "action12" --LoRA True --do_PART2 True --do_PART3 True --moral_type 'Ut' --option "CORE" --Rscaling True --r_illegal 6 --r_punishment 3 --LoRA_rank 64 --gradient_accumulation_steps 4






##################################################################
#### INFERENCE JOBS ####
##################################################################

[here the individual python jobs have been grouped within .sh scripts for ease of readability & running the evals]
[we report all evals vs a Random opponent as this displays the greatest behavioural diversity, but the scripts can be used to evaluate the fine-tuned models vs any opponent]


[This runs the core analysis - original order, plus three new permuted orders.]
[It also runs follow-up analyses with variations of the IPD prompt - unstructured, IPD-like situation, or explicitly identifying the IPD]
[the arguments here are: 
--PARTs_detail = which PART (I.e. training reward) to evaluate) 
--run_idx = the specific run/seed of training for which we want to run inference (each model is saved per seed) 
--option = naming option which was used in storing the model - this specifies the De / Ut parts (only really used in PT3 or PT4, but we need to specify it everywhere)
]

[specifying the use of new action tokens] 
inference_main.sh 	_PT2 1000 action34 COREDe
inference_main.sh 	_PT3 1000 action34 COREDe
inference_main.sh 	_PT4 1000 action34 COREDe
inference_main.sh 	_PT3after2 500 action34 COREDe 		
inference_main.sh 	_PT3 1000 action34 COREUt	
inference_main.sh	_PT3after2 500 action34 COREUt	

[For appendix: run inference with the original action tokens as in training --> particularly interesting here is the 'reversed' option, where at test time we essentially reverse the meaning of C and D tokens as compare to training time]
inference_main.sh 	_PT2 1000 action12 COREDe
inference_main.sh 	_PT3 1000 action12 COREDe
inference_main.sh 	_PT4 1000 action12 COREDe
inference_main.sh 	_PT3after2 500 action12 COREDe 		
inference_main.sh 	_PT3 1000 action12 COREUt	
inference_main.sh	_PT3after2 500 action12 COREUt	


[For appendix: run inference with only the ref model prompted to care about a certain value (as a potential benchmark)] 
inference_refmodel_preprompted.sh _PT2 1000 COREDe
inference_refmodel_preprompted.sh _PT3 1000 COREDe
inference_refmodel_preprompted.sh _PT3 1000 COREUt
inference_refmodel_preprompted.sh _PT4 1000 COREDe
inference_refmodel_preprompted.sh _PT3after2 500 COREDe 	
inference_refmodel_preprompted.sh _PT3after2 500 COREUt	



[For appendix: ask an example model follow-up questions, check with run1 only - we run this for each type of fine-tuning reward inside the .sh script]
followup_qn.sh 



